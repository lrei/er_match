{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# script for obtaining from ER a list of top X articles + a list of article URIs belonging to an event\n",
      "\n",
      "import os\n",
      "import urlparse\n",
      "import zipfile\n",
      "import json\n",
      "import datetime\n",
      "from datetime import timedelta\n",
      "from EventRegistry import *\n",
      "\n",
      "\n",
      "def url_fix(url):\n",
      "    '''basic url normalization, removes query parameters'''\n",
      "    parts= urlparse.urlparse(url)\n",
      "    return parts.scheme + parts.netloc + parts.path\n",
      "\n",
      "\n",
      "def read_tweet_file(filepath):\n",
      "    '''Read a zipped tweet file (symphony/twitter observatory archive format)'''\n",
      "    tweets = []\n",
      "    with zipfile.ZipFile(filepath, 'r') as zf:\n",
      "        name = zf.namelist()[0]\n",
      "        for line in zf.open(name):\n",
      "            tweet_json = json.loads(line)\n",
      "            tweets.append(tweet_json)\n",
      "    return tweets\n",
      "\n",
      "\n",
      "def read_tweets_from_dir(dirpath='./tweets'):\n",
      "    '''Read a directory of zipped tweet files (symphony/twitter observatory archive format)'''\n",
      "    tweets = []\n",
      "    files = os.listdir(dirpath)\n",
      "    for tweet_file in files:\n",
      "        filepath = os.path.join(dirpath, tweet_file)\n",
      "        file_tweets = read_tweet_file(filepath)\n",
      "        tweets += file_tweets\n",
      "        \n",
      "    return tweets\n",
      "\n",
      "\n",
      "def read_tweets_from_dir_gen(dirpath='../tweets'):\n",
      "    '''Generator version for reading one file at a time'''\n",
      "    files = os.listdir(dirpath)\n",
      "    for tweet_file in files:\n",
      "        filepath = os.path.join(dirpath, tweet_file)\n",
      "        yield read_tweet_file(filepath)\n",
      "\n",
      "\n",
      "def tweets_filter_with_urls(tweets):\n",
      "    '''Returns only tweets with URLs'''\n",
      "    filtered_tweets = []\n",
      "    \n",
      "    for tweet in tweets:\n",
      "        if u'urls' not in tweet['entities'] or len(tweet['entities']['urls']) == 0:\n",
      "            continue\n",
      "        \n",
      "        filtered_tweets.append(tweet)\n",
      "    \n",
      "    return filtered_tweets\n",
      "\n",
      "\n",
      "def tweets_remove_fields(tweets, allowed=[u'id_str', u'text', u'created_at']):\n",
      "    '''Removes all non-allowed fields from tweet. Adds a URL field with expanded_urls '''\n",
      "    \n",
      "    for tweet in tweets:\n",
      "        tweet_urls = tweet['entities']['urls']\n",
      "        tweet_urls = [x[u'expanded_url'] for x in tweet_urls]\n",
      "        tweet[u'urls'] = tweet_urls\n",
      "            \n",
      "        for key in tweet.keys():\n",
      "            if key not in allowed + [u'urls']:\n",
      "                del tweet[key]\n",
      "\n",
      "def tweet_day(tweet):\n",
      "    return datetime.datetime.strptime((tweet[\"created_at\"]).strip('\"'), \"%a %b %d %H:%M:%S +0000 %Y\").date()\n",
      "\n",
      "\n",
      "def get_er_urls(start=datetime.date(2014, 4, 16), end=datetime.date(2014, 4, 16)):\n",
      "    ''' get ER article URIs between a given interval of time'''\n",
      "    er = EventRegistry(host = \"http://eventregistry.org\", logging = True)\n",
      "    q = QueryArticles()\n",
      "    q.setDateLimit(start, end)\n",
      "    q.addRequestedResult(RequestArticlesUriList());\n",
      "    \n",
      "    res = er.execQuery(q)\n",
      "    obj = createStructFromDict(res)\n",
      "    urls = obj.uriList\n",
      "    urls = [url_fix(url) for url in urls]\n",
      "    return urls\n",
      "\n",
      "\n",
      "def search_er():\n",
      "    er = EventRegistry(host = \"http://eventregistry.org\", logging = True)\n",
      "\n",
      "    q = QueryEvents()\n",
      "    q.setDateLimit(datetime.date(2015, 1, 12), datetime.date(2014, 1, 12))\n",
      "    q.addRequestedResult(RequestEventsUriList())            # return uris of all events\n",
      "    res = er.execQuery(q)\n",
      "    obj = createStructFromDict(res)\n",
      "\n",
      "    langs = [\"eng\", \"deu\", \"spa\", \"slv\"];\n",
      "    size = 100;\n",
      "    for i in range(0, len(obj.uriList), size):\n",
      "        uriList = obj.uriList[i:i+size]\n",
      "        q2 = QueryEvent(uriList);\n",
      "        for lang in langs:\n",
      "            q2.addRequestedResult(RequestEventArticles(0, 2, lang = lang))  # get 10 articles about the event (any language is ok) that are closest to the center of the event\n",
      "            q2.addRequestedResult(RequestEventArticleUris());                # gives all uris for all langs \n",
      "            res2 = er.execQuery(q2)\n",
      "            obj2 = createStructFromDict(res2)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweet_files = read_tweets_from_dir_gen()\n",
      "date_cur = None\n",
      "urls = None\n",
      "er_interval = timedelta(days=3) # make this 3 days\n",
      "\n",
      "for tweets in tweet_files:\n",
      "    # remove useless tweet fields, and remove tweets without URLs\n",
      "    tweets = tweets_filter_with_urls(tweets)\n",
      "    tweets_remove_fields(tweets)\n",
      "    date_end = tweet_day(tweets[0]) # TODO: retrieve the day of this tweet (python datetime.datetime)\n",
      "    if date_end != date_cur:\n",
      "        # in this case we make a new request to ER\n",
      "        date_cur = date_end\n",
      "        date_start = date_end - er_interval # TODO: define er_interval (3days) subtract interval from date_end\n",
      "        urls = get_er_urls(date_start, date_end)\n",
      "    matched_tweets = tweet_match(tweets, urls) # TODO: string match tweet urls to ER urls \n",
      "    # TODO save to disk with tweet_id, cluster_id (CSV or JSON line delimited)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'tweet_match' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-556287b46686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdate_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mer_interval\u001b[0m \u001b[0;31m# TODO: define er_interval (3days) subtract interval from date_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_er_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmatched_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweet_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: string match tweet urls to ER urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# TODO save to disk with tweet_id, cluster_id (CSV or JSON line delimited)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'tweet_match' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(tweets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tweets[10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from datetime import datetime\n",
      "\n",
      "print created\n",
      "print type(created)\n",
      "print created + timedelta(days=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}