{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import random\n",
      "import similarity\n",
      "from collection import defaultdict\n",
      "from text_normalize import normalize_text, n_grams\n",
      "from similarity import jaccard_sim, cosine_sim, page_sim\n",
      "from ermdb import db_open, db_close, db_get, db_put\n",
      "from ermcfg import TWEET_DB_FILENAME, MATCH_URL_DB_FILENAME, ER_CENTROID_EN_DB_FILENAME\n",
      "\n",
      "print(MATCH_URL_DB_FILENAME)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/media/storage/DATA/symphony/er_match/match.url.db\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read training set from matched URLs DB\n",
      "related = {}\n",
      "\n",
      "url_match_db = db_open(MATCH_URL_DB_FILENAME, readonly=True, create=False)\n",
      "with url_match_db.begin(write=False) as txn:\n",
      "    cursor = txn.cursor()\n",
      "    for tweet_id, event_id in cursor:\n",
      "        related[tweet_id] = event_id\n",
      "db_close(url_match_db)\n",
      "print(len(related))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Opened db at /media/storage/DATA/symphony/er_match/match.url.db\n",
        "47045\n"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load Tweet text, Article Title and Article Body\n",
      "tweets = {}\n",
      "titles = {}\n",
      "bodies = {}\n",
      "missing_articles = 0\n",
      "\n",
      "tweet_db = db_open(TWEET_DB_FILENAME, create=False, readonly=True)\n",
      "article_db = db_open(ER_CENTROID_EN_DB_FILENAME, create=False, readonly=True)\n",
      "\n",
      "for tweet_id in related:\n",
      "    tweet = json.loads(db_get(tweet_db, tweet_id))[u'text']\n",
      "    \n",
      "    # get article if necessary\n",
      "    event_id = related[tweet_id]\n",
      "    if event_id in titles:\n",
      "        tweets[tweet_id] = tweet\n",
      "        continue\n",
      "    \n",
      "    article = db_get(article_db, event_id)\n",
      "    if article is None:\n",
      "        missing_articles += 1\n",
      "        continue\n",
      "    \n",
      "    tweets[tweet_id] = tweet\n",
      "        \n",
      "    article = json.loads(article)\n",
      "    \n",
      "    titles[event_id] = article['title']\n",
      "    bodies[event_id] = article['body']\n",
      "\n",
      "db_close(tweet_db)\n",
      "db_close(article_db)\n",
      "\n",
      "print('Missing articles: %d' % (missing_articles,))\n",
      "print(len(tweets))\n",
      "print(len(titles))\n",
      "print(len(bodies))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Opened db at /media/storage/DATA/symphony/er_match/tweets.db\n",
        "Opened db at /media/storage/DATA/symphony/er_match/er.en.centroid.db\n",
        "Missing articles: 0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "47045\n",
        "6154\n",
        "6154\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Preprocess text\n",
      "reload(text_normalize)\n",
      "from text_normalize import normalize_text\n",
      "\n",
      "for tweet_id in tweets:\n",
      "    tweets[tweet_id] = normalize_text(tweets[tweet_id])\n",
      "    \n",
      "for event_id in titles:\n",
      "    titles[event_id] = normalize_text(titles[event_id])\n",
      "    bodies[event_id] = normalize_text(bodies[event_id])\n",
      "\n",
      "# Remove small tweets\n",
      "for tweet_id in list(tweets.keys()):\n",
      "    if len(tweets[tweet_id].split()) < 4:\n",
      "        del tweets[tweet_id]\n",
      "        del related[tweet_id]\n",
      "        \n",
      "\n",
      "print('Preprocessed')\n",
      "print(len(related))\n",
      "print(len(tweets))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Preprocessed\n",
        "40303\n",
        "40303\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_sims(tweet_grams, title_grams, body_grams):\n",
      "    # title to tweet similarity\n",
      "    title_sim = jaccard_sim(tweet_grams, title_grams)\n",
      "    # tweet to body sims\n",
      "    body_sim_pag = page_sim(body_grams, tweet_grams)\n",
      "    body_sim_jac = jaccard_sim(tweet_grams, body_grams)\n",
      "    body_sim_cos = cosine_sim(tweet_grams, body_grams)\n",
      "    \n",
      "    return [title_sim, body_sim_pag, body_sim_jac, body_sim_cos]\n",
      "    \n",
      "\n",
      "def extract_features(tweet_text, article_title, article_body, n=4):\n",
      "    x = []\n",
      "    \n",
      "    while n > 0:\n",
      "        s = calc_sims(n_grams(tweet_text, n), n_grams(article_title, n), n_grams(article_body, n))\n",
      "        x.extend(s)\n",
      "        n -= 1\n",
      "    \n",
      "    return x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the Data Set\n",
      "x_list = []\n",
      "\n",
      "n_related = len(related)\n",
      "tweet_ids = related.keys()\n",
      "\n",
      "#  Positive Examples\n",
      "for tweet_id in tweet_ids:\n",
      "    event_id = related[tweet_id]\n",
      "    \n",
      "    tweet_text = tweets[tweet_id]\n",
      "    article_title = titles[event_id]\n",
      "    article_body = bodies[event_id]\n",
      "    \n",
      "    x_list.append(extract_features(tweet_text, article_title, article_body))\n",
      "    \n",
      "    \n",
      "# Remove Unsimilar Related\n",
      "remove = []\n",
      "tindex = 0\n",
      "for x in x_list:\n",
      "    if sum(x) == 0:\n",
      "        remove.append(tweet_ids[tindex])\n",
      "    tindex += 1\n",
      "\n",
      "tweet_ids = [x for x in tweet_ids if x not in remove]\n",
      "related = {x: related[x] for x in tweet_ids}\n",
      "\n",
      "# Rebuild Positive Examples\n",
      "x_list = []\n",
      "y_list = []\n",
      "for tweet_id in related:\n",
      "    event_id = related[tweet_id]\n",
      "    \n",
      "    tweet_text = tweets[tweet_id]\n",
      "    article_title = titles[event_id]\n",
      "    article_body = bodies[event_id]\n",
      "    \n",
      "    y_list.append(1)\n",
      "    x_list.append(extract_features(tweet_text, article_title, article_body))\n",
      "\n",
      "print(len(x_list))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "9650\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "# Create Negative (Unrelated) Set --- High Memory Use\n",
      "# @TODO Parallel?\n",
      "unrelated = {}\n",
      "zero_flag = False\n",
      "\n",
      "# all possible tweet_id: event_id combinations\n",
      "print('Create all possible combinations: (brutal ammount of memory use)')\n",
      "all_combinations = list(itertools.product(tweets.keys(), titles.keys()))\n",
      "\n",
      "# set seed in order to produce repeatable results\n",
      "random.seed(61284)\n",
      "\n",
      "print('cycle through')\n",
      "n_related = len(related)\n",
      "while len(unrelated) < n_related and len(all_combinations) > 0:\n",
      "    c_id = random.randint(0, len(all_combinations) - 1)\n",
      "    \n",
      "    tweet_id = all_combinations[c_id][0]\n",
      "    event_id = all_combinations[c_id][1]\n",
      "    del all_combinations[c_id]\n",
      "    \n",
      "    \n",
      "    # ignore related\n",
      "    if tweet_id in related and related[tweet_id] == event_id:\n",
      "        continue\n",
      "        \n",
      "    # check similarity\n",
      "    sim_vec = extract_features(tweets[tweet_id], titles[event_id], bodies[event_id])\n",
      "    sim = sum(sim_vec)\n",
      "    if sim == 0 and zero_flag:\n",
      "        continue\n",
      "    elif sim == 0 and not zero_flag:\n",
      "        zero_flag = True\n",
      "        # fall through: allows one zero similarity example\n",
      "    #else: add to unrelated\n",
      "    unrelated[tweet_id] = event_id  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import itertools\n",
      "\n",
      "# Create Negative (Unrelated) Set\n",
      "# @TODO Parallel?\n",
      "unrelated = {}\n",
      "zero_flag = False\n",
      "\n",
      "# all possible tweet_id: event_id combinations\n",
      "all_combinations = itertools.product(tweets.keys(), titles.keys())\n",
      "tried = 0\n",
      "\n",
      "n_related = len(related)\n",
      "for tweet_id, event_id in all_combinations:\n",
      "    if len(unrelated) >= n_related:\n",
      "        break\n",
      "        \n",
      "    tried += 1\n",
      "    if tried % 100000 == 0:\n",
      "        print('Tried: %d Got: %d' % (tried, len(unrelated)))\n",
      "    \n",
      "    # ignore related\n",
      "    if tweet_id in related and related[tweet_id] == event_id:\n",
      "        continue\n",
      "        \n",
      "    # check similarity\n",
      "    sim_vec = extract_features(tweets[tweet_id], titles[event_id], bodies[event_id])\n",
      "    sim = sum(sim_vec)\n",
      "    if sim == 0 and zero_flag:\n",
      "        continue\n",
      "    elif sim == 0 and not zero_flag:\n",
      "        zero_flag = True\n",
      "        # fall through: allows one zero similarity example\n",
      "    #else: add to unrelated\n",
      "    unrelated[tweet_id] = event_id   \n",
      "\n",
      "# Rebalance dataset if necessary\n",
      "n_unrelated = len(unrelated)\n",
      "if n_unrelated < n_related:\n",
      "    print('Rebalancing Related %d -> %d' % (n_related, n_unrelated))\n",
      "    x_list = x_list[0:n_unrelated]\n",
      "    y_list = y_list[0:n_unrelated]\n",
      "    n_related = n_unrelated\n",
      "    \n",
      "    \n",
      "# Build Negative Examples\n",
      "print('Building Negative Examples')\n",
      "tweet_ids = unrelated.keys()\n",
      "for tweet_id in tweet_ids:\n",
      "    event_id = unrelated[tweet_id]\n",
      "    \n",
      "    tweet_text = tweets[tweet_id]\n",
      "    article_title = titles[event_id]\n",
      "    article_body = bodies[event_id]\n",
      "    \n",
      "    y_list.append(0)\n",
      "    x_list.append(extract_features(tweet_text, article_title, article_body))\n",
      "\n",
      "print('Related examples: %d' % (n_related,))\n",
      "print('Related tweets (before rebalancing): %d' % (len(set(related.keys())),))\n",
      "print('Related events (before rebalancing): %d' % (len(set(related.values())),))\n",
      "print('Unrelated examples: %d' % (n_unrelated,))\n",
      "print('Unrelated tweets: %d' % (len(set(unrelated.keys())),))\n",
      "print('Unelated events: %d' % (len(set(unrelated.values())),))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Create all possible combinations\n",
        "cycle through"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_related = len(related)\n",
      "n_unrelated = len(unrelated)\n",
      "\n",
      "sim_related = 0.0\n",
      "sim_unrelated = 0.0\n",
      "\n",
      "empty_related = 0\n",
      "for ii in range(0, n_related):\n",
      "    if sum(x_list[ii]) == 0:\n",
      "        empty_related += 1\n",
      "    sim_related += sum(x_list[ii])\n",
      "print('Empty Related = %d / %d' % (empty_related, n_related))\n",
      "print('Total Related sim = %f' % (sim_related,))\n",
      "\n",
      "empty_unrelated = 0\n",
      "for ii in range(n_related, n_related + n_unrelated):\n",
      "    if sum(x_list[ii]) == 0:\n",
      "        empty_unrelated += 1\n",
      "    sim_unrelated += sum(x_list[ii])\n",
      "print('Empty Unrelated = %d / %d' % (empty_unrelated, n_unrelated))\n",
      "print('Total Unrelated sim = %f' % (sim_unrelated,))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Empty Related = 0 / 9650\n",
        "Total Related sim = 65979.177020\n",
        "Empty Unrelated = 6705 / 9650\n",
        "Total Unrelated sim = 10385.206319\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}